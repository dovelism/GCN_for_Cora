{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.nn as pyg\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import Node2Vec\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(folder=\"/Users/mac/Desktop/Coding/GraphCNN/cora\" , data_name = \"cora\"):\n",
    "    data = Planetoid(root=folder ,name=data_name,  \n",
    "              pre_transform=T.KNNGraph(k=6) ,transform=T.NormalizeFeatures()  )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create GCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyGCNN,self).__init__()\n",
    "        \n",
    "        self.conv1 = pyg.GCNConv(data.num_features,300 ,cached=True)\n",
    "        #self.mlp = nn.Linear(300, 300)\n",
    "        self.conv2 = pyg.GCNConv(300, data.num_classes,cached=True)\n",
    "        self.reg_params = self.conv1.parameters()\n",
    "        self.non_reg_params = self.conv2.parameters()\n",
    "        #self.mlp = nn.Linear(300, data.num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self,data):\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_weight = data.edge_attr\n",
    "        hid = self.conv1(x=x, edge_index=edge_index,edge_weight = data.edge_attr)\n",
    "        hid = F.relu(hid)\n",
    "        hid = F.dropout(hid, training=self.training)\n",
    "        #hid = self.mlp(hid)\n",
    "        hid = self.conv2(x=hid ,edge_index=edge_index,edge_weight = data.edge_attr)\n",
    "        #hid = F.relu(hid)\n",
    "        out = F.log_softmax(hid,dim=1)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "    cora = get_data()\n",
    "    \n",
    "\n",
    "    NET = MyGCNN()\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    NET = NET.to(device)\n",
    "    cora = cora[0].to(device)\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(NET.parameters(), lr=1e-3)   \n",
    "    #optimizer = torch.optim.Adam([dict(params=NET.reg_params, weight_decay=5e-4),\n",
    "                                  #dict(params=NET.non_reg_params, weight_decay=5e-4)], lr=1e-3)\n",
    "    optimizer = torch.optim.Adagrad([\n",
    "        dict(params=NET.reg_params, weight_decay=5e-4),\n",
    "        dict(params=NET.non_reg_params, weight_decay=5e-4)], \n",
    "        lr_decay=0.01, weight_decay=0.01, initial_accumulator_value=0)\n",
    "\n",
    "\n",
    "    NET.train()\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        output = NET(cora)\n",
    "        loss = F.nll_loss(output[cora.train_mask] , cora.y[cora.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"epoch:\",epoch+1 ,\"loss:\",loss.item())\n",
    "    \n",
    "    NET.eval()\n",
    "    _ , prediction = NET(cora).max(dim=1)\n",
    "    target = cora.y\n",
    "    test_correct = prediction[cora.test_mask].eq(target[cora.test_mask]).sum().item()\n",
    "    test_number = cora.test_mask.sum().item()\n",
    "    \n",
    "    print(\"Accuracy of Test Samples:\" , test_correct / test_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 1.9459202289581299\n",
      "epoch: 2 loss: 1.9224953651428223\n",
      "epoch: 3 loss: 1.8948427438735962\n",
      "epoch: 4 loss: 1.864618182182312\n",
      "epoch: 5 loss: 1.831382155418396\n",
      "epoch: 6 loss: 1.7977546453475952\n",
      "epoch: 7 loss: 1.7686859369277954\n",
      "epoch: 8 loss: 1.7363632917404175\n",
      "epoch: 9 loss: 1.7024465799331665\n",
      "epoch: 10 loss: 1.6704761981964111\n",
      "epoch: 11 loss: 1.6396540403366089\n",
      "epoch: 12 loss: 1.6129411458969116\n",
      "epoch: 13 loss: 1.5808777809143066\n",
      "epoch: 14 loss: 1.5539730787277222\n",
      "epoch: 15 loss: 1.5252199172973633\n",
      "epoch: 16 loss: 1.4982720613479614\n",
      "epoch: 17 loss: 1.4743413925170898\n",
      "epoch: 18 loss: 1.4519122838974\n",
      "epoch: 19 loss: 1.4266326427459717\n",
      "epoch: 20 loss: 1.3988780975341797\n",
      "epoch: 21 loss: 1.3768144845962524\n",
      "epoch: 22 loss: 1.3569560050964355\n",
      "epoch: 23 loss: 1.327013611793518\n",
      "epoch: 24 loss: 1.3127880096435547\n",
      "epoch: 25 loss: 1.2881333827972412\n",
      "epoch: 26 loss: 1.2694188356399536\n",
      "epoch: 27 loss: 1.2590000629425049\n",
      "epoch: 28 loss: 1.2274465560913086\n",
      "epoch: 29 loss: 1.217475175857544\n",
      "epoch: 30 loss: 1.2086352109909058\n",
      "epoch: 31 loss: 1.189636468887329\n",
      "epoch: 32 loss: 1.1734981536865234\n",
      "epoch: 33 loss: 1.1570172309875488\n",
      "epoch: 34 loss: 1.1446977853775024\n",
      "epoch: 35 loss: 1.1235548257827759\n",
      "epoch: 36 loss: 1.1158455610275269\n",
      "epoch: 37 loss: 1.1001380681991577\n",
      "epoch: 38 loss: 1.0888906717300415\n",
      "epoch: 39 loss: 1.0805753469467163\n",
      "epoch: 40 loss: 1.073439359664917\n",
      "epoch: 41 loss: 1.0553184747695923\n",
      "epoch: 42 loss: 1.0349465608596802\n",
      "epoch: 43 loss: 1.0420271158218384\n",
      "epoch: 44 loss: 1.0186502933502197\n",
      "epoch: 45 loss: 1.006453514099121\n",
      "epoch: 46 loss: 0.9943327903747559\n",
      "epoch: 47 loss: 0.9825701117515564\n",
      "epoch: 48 loss: 0.9845491647720337\n",
      "epoch: 49 loss: 0.9821665287017822\n",
      "epoch: 50 loss: 0.9652536511421204\n",
      "epoch: 51 loss: 0.9528799057006836\n",
      "epoch: 52 loss: 0.9472084045410156\n",
      "epoch: 53 loss: 0.9446545243263245\n",
      "epoch: 54 loss: 0.9293376207351685\n",
      "epoch: 55 loss: 0.9275789260864258\n",
      "epoch: 56 loss: 0.9116494059562683\n",
      "epoch: 57 loss: 0.9139159321784973\n",
      "epoch: 58 loss: 0.903693437576294\n",
      "epoch: 59 loss: 0.9040122628211975\n",
      "epoch: 60 loss: 0.8930876851081848\n",
      "epoch: 61 loss: 0.8787840604782104\n",
      "epoch: 62 loss: 0.8797941207885742\n",
      "epoch: 63 loss: 0.8624579310417175\n",
      "epoch: 64 loss: 0.8703861832618713\n",
      "epoch: 65 loss: 0.8555546402931213\n",
      "epoch: 66 loss: 0.8471471667289734\n",
      "epoch: 67 loss: 0.8468236923217773\n",
      "epoch: 68 loss: 0.8401148915290833\n",
      "epoch: 69 loss: 0.8455018401145935\n",
      "epoch: 70 loss: 0.838752269744873\n",
      "epoch: 71 loss: 0.8219575881958008\n",
      "epoch: 72 loss: 0.820995569229126\n",
      "epoch: 73 loss: 0.8137627840042114\n",
      "epoch: 74 loss: 0.8106883764266968\n",
      "epoch: 75 loss: 0.8116694092750549\n",
      "epoch: 76 loss: 0.8074520826339722\n",
      "epoch: 77 loss: 0.8050565719604492\n",
      "epoch: 78 loss: 0.7912735342979431\n",
      "epoch: 79 loss: 0.7845616936683655\n",
      "epoch: 80 loss: 0.7836442589759827\n",
      "epoch: 81 loss: 0.785037100315094\n",
      "epoch: 82 loss: 0.7834660410881042\n",
      "epoch: 83 loss: 0.7690733671188354\n",
      "epoch: 84 loss: 0.7614206075668335\n",
      "epoch: 85 loss: 0.7602397799491882\n",
      "epoch: 86 loss: 0.7613285183906555\n",
      "epoch: 87 loss: 0.7651704549789429\n",
      "epoch: 88 loss: 0.75223308801651\n",
      "epoch: 89 loss: 0.7404904365539551\n",
      "epoch: 90 loss: 0.7466719150543213\n",
      "epoch: 91 loss: 0.7405018210411072\n",
      "epoch: 92 loss: 0.743904173374176\n",
      "epoch: 93 loss: 0.728614091873169\n",
      "epoch: 94 loss: 0.7290499806404114\n",
      "epoch: 95 loss: 0.7324013113975525\n",
      "epoch: 96 loss: 0.7328881025314331\n",
      "epoch: 97 loss: 0.720995306968689\n",
      "epoch: 98 loss: 0.7211900949478149\n",
      "epoch: 99 loss: 0.7187830209732056\n",
      "epoch: 100 loss: 0.7217695713043213\n",
      "epoch: 101 loss: 0.7027285099029541\n",
      "epoch: 102 loss: 0.7112447023391724\n",
      "epoch: 103 loss: 0.7072012424468994\n",
      "epoch: 104 loss: 0.7037107348442078\n",
      "epoch: 105 loss: 0.705159604549408\n",
      "epoch: 106 loss: 0.7039617896080017\n",
      "epoch: 107 loss: 0.6881154775619507\n",
      "epoch: 108 loss: 0.6910114884376526\n",
      "epoch: 109 loss: 0.6867230534553528\n",
      "epoch: 110 loss: 0.6941681504249573\n",
      "epoch: 111 loss: 0.6829382181167603\n",
      "epoch: 112 loss: 0.6869129538536072\n",
      "epoch: 113 loss: 0.6909164190292358\n",
      "epoch: 114 loss: 0.6792399287223816\n",
      "epoch: 115 loss: 0.678680956363678\n",
      "epoch: 116 loss: 0.6810980439186096\n",
      "epoch: 117 loss: 0.6707653403282166\n",
      "epoch: 118 loss: 0.6695727109909058\n",
      "epoch: 119 loss: 0.6575447916984558\n",
      "epoch: 120 loss: 0.6663942337036133\n",
      "epoch: 121 loss: 0.6604471802711487\n",
      "epoch: 122 loss: 0.6649134755134583\n",
      "epoch: 123 loss: 0.6640661954879761\n",
      "epoch: 124 loss: 0.6583274602890015\n",
      "epoch: 125 loss: 0.6536726951599121\n",
      "epoch: 126 loss: 0.6549918055534363\n",
      "epoch: 127 loss: 0.6537570953369141\n",
      "epoch: 128 loss: 0.6392717957496643\n",
      "epoch: 129 loss: 0.6467407941818237\n",
      "epoch: 130 loss: 0.6409475803375244\n",
      "epoch: 131 loss: 0.6404979825019836\n",
      "epoch: 132 loss: 0.6493799090385437\n",
      "epoch: 133 loss: 0.6436341404914856\n",
      "epoch: 134 loss: 0.638349711894989\n",
      "epoch: 135 loss: 0.6412529349327087\n",
      "epoch: 136 loss: 0.631068229675293\n",
      "epoch: 137 loss: 0.6399194002151489\n",
      "epoch: 138 loss: 0.6258156299591064\n",
      "epoch: 139 loss: 0.6205419898033142\n",
      "epoch: 140 loss: 0.6256194114685059\n",
      "epoch: 141 loss: 0.625086784362793\n",
      "epoch: 142 loss: 0.6231958866119385\n",
      "epoch: 143 loss: 0.6214301586151123\n",
      "epoch: 144 loss: 0.6261441707611084\n",
      "epoch: 145 loss: 0.6160919070243835\n",
      "epoch: 146 loss: 0.6253159046173096\n",
      "epoch: 147 loss: 0.6218565106391907\n",
      "epoch: 148 loss: 0.6151554584503174\n",
      "epoch: 149 loss: 0.619909405708313\n",
      "epoch: 150 loss: 0.6031420826911926\n",
      "epoch: 151 loss: 0.6124980449676514\n",
      "epoch: 152 loss: 0.6122947931289673\n",
      "epoch: 153 loss: 0.608999490737915\n",
      "epoch: 154 loss: 0.6083663702011108\n",
      "epoch: 155 loss: 0.6111234426498413\n",
      "epoch: 156 loss: 0.6018058061599731\n",
      "epoch: 157 loss: 0.606636643409729\n",
      "epoch: 158 loss: 0.604499340057373\n",
      "epoch: 159 loss: 0.6000566482543945\n",
      "epoch: 160 loss: 0.6012890338897705\n",
      "epoch: 161 loss: 0.5832988619804382\n",
      "epoch: 162 loss: 0.60219407081604\n",
      "epoch: 163 loss: 0.5978268384933472\n",
      "epoch: 164 loss: 0.5953314900398254\n",
      "epoch: 165 loss: 0.6034220457077026\n",
      "epoch: 166 loss: 0.5922693014144897\n",
      "epoch: 167 loss: 0.5932801961898804\n",
      "epoch: 168 loss: 0.5958635210990906\n",
      "epoch: 169 loss: 0.5942419171333313\n",
      "epoch: 170 loss: 0.586283802986145\n",
      "epoch: 171 loss: 0.5820310115814209\n",
      "epoch: 172 loss: 0.5893784165382385\n",
      "epoch: 173 loss: 0.5926177501678467\n",
      "epoch: 174 loss: 0.5847956538200378\n",
      "epoch: 175 loss: 0.5902775526046753\n",
      "epoch: 176 loss: 0.5886574387550354\n",
      "epoch: 177 loss: 0.5767465233802795\n",
      "epoch: 178 loss: 0.5762442350387573\n",
      "epoch: 179 loss: 0.5843843221664429\n",
      "epoch: 180 loss: 0.5744171738624573\n",
      "epoch: 181 loss: 0.5847927927970886\n",
      "epoch: 182 loss: 0.5848539471626282\n",
      "epoch: 183 loss: 0.5831449031829834\n",
      "epoch: 184 loss: 0.5758756399154663\n",
      "epoch: 185 loss: 0.5717942714691162\n",
      "epoch: 186 loss: 0.568439781665802\n",
      "epoch: 187 loss: 0.5670661926269531\n",
      "epoch: 188 loss: 0.5769717693328857\n",
      "epoch: 189 loss: 0.5737143158912659\n",
      "epoch: 190 loss: 0.571692168712616\n",
      "epoch: 191 loss: 0.5723066329956055\n",
      "epoch: 192 loss: 0.5683495402336121\n",
      "epoch: 193 loss: 0.563023567199707\n",
      "epoch: 194 loss: 0.5609415769577026\n",
      "epoch: 195 loss: 0.5606578588485718\n",
      "epoch: 196 loss: 0.5651735067367554\n",
      "epoch: 197 loss: 0.5540450215339661\n",
      "epoch: 198 loss: 0.5541375875473022\n",
      "epoch: 199 loss: 0.5532822608947754\n",
      "epoch: 200 loss: 0.5523686408996582\n",
      "epoch: 201 loss: 0.5615448355674744\n",
      "epoch: 202 loss: 0.5536192059516907\n",
      "epoch: 203 loss: 0.5589578747749329\n",
      "epoch: 204 loss: 0.5610225200653076\n",
      "epoch: 205 loss: 0.5549089312553406\n",
      "epoch: 206 loss: 0.5474112629890442\n",
      "epoch: 207 loss: 0.5454838275909424\n",
      "epoch: 208 loss: 0.5473019480705261\n",
      "epoch: 209 loss: 0.553836464881897\n",
      "epoch: 210 loss: 0.5490727424621582\n",
      "epoch: 211 loss: 0.5546950697898865\n",
      "epoch: 212 loss: 0.5462743043899536\n",
      "epoch: 213 loss: 0.540723443031311\n",
      "epoch: 214 loss: 0.5482940077781677\n",
      "epoch: 215 loss: 0.5510451197624207\n",
      "epoch: 216 loss: 0.5424669981002808\n",
      "epoch: 217 loss: 0.5447571873664856\n",
      "epoch: 218 loss: 0.5548095703125\n",
      "epoch: 219 loss: 0.5474801063537598\n",
      "epoch: 220 loss: 0.5443283319473267\n",
      "epoch: 221 loss: 0.5347703099250793\n",
      "epoch: 222 loss: 0.5473277568817139\n",
      "epoch: 223 loss: 0.5342850685119629\n",
      "epoch: 224 loss: 0.5332987308502197\n",
      "epoch: 225 loss: 0.5304041504859924\n",
      "epoch: 226 loss: 0.5362703204154968\n",
      "epoch: 227 loss: 0.5373550057411194\n",
      "epoch: 228 loss: 0.5354069471359253\n",
      "epoch: 229 loss: 0.5426144003868103\n",
      "epoch: 230 loss: 0.5424101948738098\n",
      "epoch: 231 loss: 0.5368907451629639\n",
      "epoch: 232 loss: 0.531834602355957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 233 loss: 0.5311967134475708\n",
      "epoch: 234 loss: 0.5252360701560974\n",
      "epoch: 235 loss: 0.5280539393424988\n",
      "epoch: 236 loss: 0.5325965881347656\n",
      "epoch: 237 loss: 0.5341835021972656\n",
      "epoch: 238 loss: 0.5312906503677368\n",
      "epoch: 239 loss: 0.5261034369468689\n",
      "epoch: 240 loss: 0.530067503452301\n",
      "epoch: 241 loss: 0.5187565088272095\n",
      "epoch: 242 loss: 0.5277328491210938\n",
      "epoch: 243 loss: 0.5263313055038452\n",
      "epoch: 244 loss: 0.5240254402160645\n",
      "epoch: 245 loss: 0.5315849184989929\n",
      "epoch: 246 loss: 0.5195696353912354\n",
      "epoch: 247 loss: 0.5247431993484497\n",
      "epoch: 248 loss: 0.5203742980957031\n",
      "epoch: 249 loss: 0.5271813273429871\n",
      "epoch: 250 loss: 0.5263353586196899\n",
      "epoch: 251 loss: 0.5187357068061829\n",
      "epoch: 252 loss: 0.523711621761322\n",
      "epoch: 253 loss: 0.5206085443496704\n",
      "epoch: 254 loss: 0.5206627249717712\n",
      "epoch: 255 loss: 0.5170743465423584\n",
      "epoch: 256 loss: 0.5219786763191223\n",
      "epoch: 257 loss: 0.5189081430435181\n",
      "epoch: 258 loss: 0.5114572048187256\n",
      "epoch: 259 loss: 0.509066641330719\n",
      "epoch: 260 loss: 0.5209947228431702\n",
      "epoch: 261 loss: 0.5093979239463806\n",
      "epoch: 262 loss: 0.5142092704772949\n",
      "epoch: 263 loss: 0.516200065612793\n",
      "epoch: 264 loss: 0.5113909840583801\n",
      "epoch: 265 loss: 0.5205113291740417\n",
      "epoch: 266 loss: 0.5137009024620056\n",
      "epoch: 267 loss: 0.5209360122680664\n",
      "epoch: 268 loss: 0.5140367150306702\n",
      "epoch: 269 loss: 0.5056028962135315\n",
      "epoch: 270 loss: 0.5087468028068542\n",
      "epoch: 271 loss: 0.5099827647209167\n",
      "epoch: 272 loss: 0.5033079981803894\n",
      "epoch: 273 loss: 0.5133411884307861\n",
      "epoch: 274 loss: 0.5150657892227173\n",
      "epoch: 275 loss: 0.5050637722015381\n",
      "epoch: 276 loss: 0.5117771029472351\n",
      "epoch: 277 loss: 0.5050557851791382\n",
      "epoch: 278 loss: 0.5093176960945129\n",
      "epoch: 279 loss: 0.493982195854187\n",
      "epoch: 280 loss: 0.5081095695495605\n",
      "epoch: 281 loss: 0.5137450695037842\n",
      "epoch: 282 loss: 0.5011914372444153\n",
      "epoch: 283 loss: 0.5075764060020447\n",
      "epoch: 284 loss: 0.4988434910774231\n",
      "epoch: 285 loss: 0.5149874687194824\n",
      "epoch: 286 loss: 0.49506568908691406\n",
      "epoch: 287 loss: 0.5031994581222534\n",
      "epoch: 288 loss: 0.5060989260673523\n",
      "epoch: 289 loss: 0.5034863948822021\n",
      "epoch: 290 loss: 0.4957839548587799\n",
      "epoch: 291 loss: 0.49776774644851685\n",
      "epoch: 292 loss: 0.49928349256515503\n",
      "epoch: 293 loss: 0.5025004148483276\n",
      "epoch: 294 loss: 0.49778661131858826\n",
      "epoch: 295 loss: 0.5002532005310059\n",
      "epoch: 296 loss: 0.5077411532402039\n",
      "epoch: 297 loss: 0.5058763027191162\n",
      "epoch: 298 loss: 0.5048964619636536\n",
      "epoch: 299 loss: 0.49953001737594604\n",
      "epoch: 300 loss: 0.5057076811790466\n",
      "epoch: 301 loss: 0.5007869601249695\n",
      "epoch: 302 loss: 0.5024725794792175\n",
      "epoch: 303 loss: 0.48586976528167725\n",
      "epoch: 304 loss: 0.5016216039657593\n",
      "epoch: 305 loss: 0.49636638164520264\n",
      "epoch: 306 loss: 0.4977574050426483\n",
      "epoch: 307 loss: 0.5001239776611328\n",
      "epoch: 308 loss: 0.4929908812046051\n",
      "epoch: 309 loss: 0.5030893683433533\n",
      "epoch: 310 loss: 0.4976534843444824\n",
      "epoch: 311 loss: 0.4966456890106201\n",
      "epoch: 312 loss: 0.4855298101902008\n",
      "epoch: 313 loss: 0.48946821689605713\n",
      "epoch: 314 loss: 0.48263663053512573\n",
      "epoch: 315 loss: 0.49599489569664\n",
      "epoch: 316 loss: 0.48263123631477356\n",
      "epoch: 317 loss: 0.4966364800930023\n",
      "epoch: 318 loss: 0.48210203647613525\n",
      "epoch: 319 loss: 0.4981139600276947\n",
      "epoch: 320 loss: 0.48964712023735046\n",
      "epoch: 321 loss: 0.4817623794078827\n",
      "epoch: 322 loss: 0.4926009476184845\n",
      "epoch: 323 loss: 0.4829551577568054\n",
      "epoch: 324 loss: 0.4990706443786621\n",
      "epoch: 325 loss: 0.4955502152442932\n",
      "epoch: 326 loss: 0.4828203022480011\n",
      "epoch: 327 loss: 0.48688316345214844\n",
      "epoch: 328 loss: 0.4855043590068817\n",
      "epoch: 329 loss: 0.48303380608558655\n",
      "epoch: 330 loss: 0.480499267578125\n",
      "epoch: 331 loss: 0.4862259328365326\n",
      "epoch: 332 loss: 0.4849451780319214\n",
      "epoch: 333 loss: 0.4842831790447235\n",
      "epoch: 334 loss: 0.48429086804389954\n",
      "epoch: 335 loss: 0.4833083152770996\n",
      "epoch: 336 loss: 0.48225605487823486\n",
      "epoch: 337 loss: 0.4855179786682129\n",
      "epoch: 338 loss: 0.47999244928359985\n",
      "epoch: 339 loss: 0.48612770438194275\n",
      "epoch: 340 loss: 0.47689682245254517\n",
      "epoch: 341 loss: 0.4759369492530823\n",
      "epoch: 342 loss: 0.48441362380981445\n",
      "epoch: 343 loss: 0.4844966232776642\n",
      "epoch: 344 loss: 0.48031142354011536\n",
      "epoch: 345 loss: 0.4757462739944458\n",
      "epoch: 346 loss: 0.4941035807132721\n",
      "epoch: 347 loss: 0.46246403455734253\n",
      "epoch: 348 loss: 0.4829382300376892\n",
      "epoch: 349 loss: 0.4789716303348541\n",
      "epoch: 350 loss: 0.4792855381965637\n",
      "epoch: 351 loss: 0.46811643242836\n",
      "epoch: 352 loss: 0.4808988869190216\n",
      "epoch: 353 loss: 0.4702660143375397\n",
      "epoch: 354 loss: 0.4843336343765259\n",
      "epoch: 355 loss: 0.47956836223602295\n",
      "epoch: 356 loss: 0.4671248197555542\n",
      "epoch: 357 loss: 0.4808860719203949\n",
      "epoch: 358 loss: 0.47517192363739014\n",
      "epoch: 359 loss: 0.4719313085079193\n",
      "epoch: 360 loss: 0.4726172387599945\n",
      "epoch: 361 loss: 0.4685521125793457\n",
      "epoch: 362 loss: 0.4765317142009735\n",
      "epoch: 363 loss: 0.46725568175315857\n",
      "epoch: 364 loss: 0.47820958495140076\n",
      "epoch: 365 loss: 0.48116445541381836\n",
      "epoch: 366 loss: 0.4627280533313751\n",
      "epoch: 367 loss: 0.47564902901649475\n",
      "epoch: 368 loss: 0.4633197784423828\n",
      "epoch: 369 loss: 0.47585153579711914\n",
      "epoch: 370 loss: 0.4759373366832733\n",
      "epoch: 371 loss: 0.46148207783699036\n",
      "epoch: 372 loss: 0.4745608866214752\n",
      "epoch: 373 loss: 0.477361798286438\n",
      "epoch: 374 loss: 0.467828631401062\n",
      "epoch: 375 loss: 0.4706805646419525\n",
      "epoch: 376 loss: 0.46685513854026794\n",
      "epoch: 377 loss: 0.46692776679992676\n",
      "epoch: 378 loss: 0.4655294716358185\n",
      "epoch: 379 loss: 0.47376060485839844\n",
      "epoch: 380 loss: 0.474645733833313\n",
      "epoch: 381 loss: 0.47614210844039917\n",
      "epoch: 382 loss: 0.46502161026000977\n",
      "epoch: 383 loss: 0.470251202583313\n",
      "epoch: 384 loss: 0.461311936378479\n",
      "epoch: 385 loss: 0.47250694036483765\n",
      "epoch: 386 loss: 0.47053322196006775\n",
      "epoch: 387 loss: 0.47234007716178894\n",
      "epoch: 388 loss: 0.46485215425491333\n",
      "epoch: 389 loss: 0.47158998250961304\n",
      "epoch: 390 loss: 0.4730014204978943\n",
      "epoch: 391 loss: 0.4598407745361328\n",
      "epoch: 392 loss: 0.47728079557418823\n",
      "epoch: 393 loss: 0.45942240953445435\n",
      "epoch: 394 loss: 0.4788767099380493\n",
      "epoch: 395 loss: 0.4567973017692566\n",
      "epoch: 396 loss: 0.46287813782691956\n",
      "epoch: 397 loss: 0.4612160325050354\n",
      "epoch: 398 loss: 0.4641132354736328\n",
      "epoch: 399 loss: 0.4677813947200775\n",
      "epoch: 400 loss: 0.4663844108581543\n",
      "epoch: 401 loss: 0.4479084312915802\n",
      "epoch: 402 loss: 0.47593361139297485\n",
      "epoch: 403 loss: 0.4593932628631592\n",
      "epoch: 404 loss: 0.4718216061592102\n",
      "epoch: 405 loss: 0.4511142075061798\n",
      "epoch: 406 loss: 0.45571741461753845\n",
      "epoch: 407 loss: 0.4630976617336273\n",
      "epoch: 408 loss: 0.4621286690235138\n",
      "epoch: 409 loss: 0.46867766976356506\n",
      "epoch: 410 loss: 0.4765188992023468\n",
      "epoch: 411 loss: 0.4615413248538971\n",
      "epoch: 412 loss: 0.4488671123981476\n",
      "epoch: 413 loss: 0.4516005516052246\n",
      "epoch: 414 loss: 0.46403568983078003\n",
      "epoch: 415 loss: 0.45991837978363037\n",
      "epoch: 416 loss: 0.46465709805488586\n",
      "epoch: 417 loss: 0.4632907211780548\n",
      "epoch: 418 loss: 0.4581183195114136\n",
      "epoch: 419 loss: 0.44799530506134033\n",
      "epoch: 420 loss: 0.46645498275756836\n",
      "epoch: 421 loss: 0.45273444056510925\n",
      "epoch: 422 loss: 0.4618850648403168\n",
      "epoch: 423 loss: 0.466055691242218\n",
      "epoch: 424 loss: 0.45207148790359497\n",
      "epoch: 425 loss: 0.4577297866344452\n",
      "epoch: 426 loss: 0.4573310911655426\n",
      "epoch: 427 loss: 0.4563940167427063\n",
      "epoch: 428 loss: 0.45694220066070557\n",
      "epoch: 429 loss: 0.4550967216491699\n",
      "epoch: 430 loss: 0.4487374424934387\n",
      "epoch: 431 loss: 0.450215607881546\n",
      "epoch: 432 loss: 0.46173813939094543\n",
      "epoch: 433 loss: 0.462309867143631\n",
      "epoch: 434 loss: 0.46304813027381897\n",
      "epoch: 435 loss: 0.4531628489494324\n",
      "epoch: 436 loss: 0.45369765162467957\n",
      "epoch: 437 loss: 0.45073822140693665\n",
      "epoch: 438 loss: 0.4543464481830597\n",
      "epoch: 439 loss: 0.45804089307785034\n",
      "epoch: 440 loss: 0.4591979384422302\n",
      "epoch: 441 loss: 0.4501069486141205\n",
      "epoch: 442 loss: 0.44721782207489014\n",
      "epoch: 443 loss: 0.45154181122779846\n",
      "epoch: 444 loss: 0.45313921570777893\n",
      "epoch: 445 loss: 0.4538987874984741\n",
      "epoch: 446 loss: 0.4508851170539856\n",
      "epoch: 447 loss: 0.4522168040275574\n",
      "epoch: 448 loss: 0.4534640610218048\n",
      "epoch: 449 loss: 0.447397381067276\n",
      "epoch: 450 loss: 0.4585552215576172\n",
      "epoch: 451 loss: 0.453546404838562\n",
      "epoch: 452 loss: 0.45513370633125305\n",
      "epoch: 453 loss: 0.4511542320251465\n",
      "epoch: 454 loss: 0.44422805309295654\n",
      "epoch: 455 loss: 0.46590694785118103\n",
      "epoch: 456 loss: 0.4529474377632141\n",
      "epoch: 457 loss: 0.45202961564064026\n",
      "epoch: 458 loss: 0.4476270079612732\n",
      "epoch: 459 loss: 0.45248326659202576\n",
      "epoch: 460 loss: 0.44819769263267517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 461 loss: 0.44929084181785583\n",
      "epoch: 462 loss: 0.44436562061309814\n",
      "epoch: 463 loss: 0.4389514625072479\n",
      "epoch: 464 loss: 0.44688573479652405\n",
      "epoch: 465 loss: 0.4477388560771942\n",
      "epoch: 466 loss: 0.4560379981994629\n",
      "epoch: 467 loss: 0.44700273871421814\n",
      "epoch: 468 loss: 0.45966359972953796\n",
      "epoch: 469 loss: 0.44719260931015015\n",
      "epoch: 470 loss: 0.43772509694099426\n",
      "epoch: 471 loss: 0.4478265643119812\n",
      "epoch: 472 loss: 0.45769059658050537\n",
      "epoch: 473 loss: 0.4400230050086975\n",
      "epoch: 474 loss: 0.45191189646720886\n",
      "epoch: 475 loss: 0.4452357292175293\n",
      "epoch: 476 loss: 0.44270187616348267\n",
      "epoch: 477 loss: 0.440834105014801\n",
      "epoch: 478 loss: 0.46138882637023926\n",
      "epoch: 479 loss: 0.43824514746665955\n",
      "epoch: 480 loss: 0.43535518646240234\n",
      "epoch: 481 loss: 0.4422002136707306\n",
      "epoch: 482 loss: 0.44310492277145386\n",
      "epoch: 483 loss: 0.4489981234073639\n",
      "epoch: 484 loss: 0.4501461982727051\n",
      "epoch: 485 loss: 0.45192384719848633\n",
      "epoch: 486 loss: 0.44399434328079224\n",
      "epoch: 487 loss: 0.45210298895835876\n",
      "epoch: 488 loss: 0.4488977789878845\n",
      "epoch: 489 loss: 0.4443909525871277\n",
      "epoch: 490 loss: 0.4489089250564575\n",
      "epoch: 491 loss: 0.4462681710720062\n",
      "epoch: 492 loss: 0.44990241527557373\n",
      "epoch: 493 loss: 0.4469048082828522\n",
      "epoch: 494 loss: 0.44187384843826294\n",
      "epoch: 495 loss: 0.4371051490306854\n",
      "epoch: 496 loss: 0.443855881690979\n",
      "epoch: 497 loss: 0.4435575008392334\n",
      "epoch: 498 loss: 0.44492092728614807\n",
      "epoch: 499 loss: 0.43208032846450806\n",
      "epoch: 500 loss: 0.4396163523197174\n",
      "Accuracy of Test Samples: 0.824\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
